# -*- coding: utf-8 -*-
"""Resume Tagger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g8_iL9PgaqLRZA7IfADQP2vANfcYB_9E
"""

import pandas as pd
df=pd.read_csv('UpdatedResumeDataSet.csv',nrows=900)

df.head(1)

df['Resume']=df['Resume'].str.lower()
df['Category']=df['Category'].str.lower()
df.head()

def remove_whitespace(Resume):
    return  " ".join(Resume.split())

df['Resume']=df['Resume'].apply(remove_whitespace)

pip install nltk

import nltk

nltk.download('all')

from nltk import word_tokenize
df['Resume']=df ['Resume'].apply(lambda X: word_tokenize(X))
df.head()

nltk.download('stopwords')
from nltk.corpus import stopwords
",".join(stopwords.words('english'))
stop_words=set(stopwords.words('english'))

def remove_stop(x):
  return ",".join([word for word in str(x).split() if word not in stop_words])
  df['Resume']=df['Resume'].apply(lambda x: remove_stop(x))

df.head()

import string

punct=string.punctuation

def remove_punct(x):
  return x.translate(str.maketrans("","",punct))
  df['Resume']=df['Resume'].apply(lambda x: remove_punct(x))

df.head()

"""removal of frequent words"""

from nltk import FreqDist

def frequent_words(df):

    lst=[]
    for text in df.values:
        lst+=text[0]
    fdist=FreqDist(lst)
    return fdist.most_common(10)
frequent_words(df)

freq_words = frequent_words(df)

lst = []
for a,b in freq_words:
    lst.append(b)

def remove_freq_words(text):

    result=[]
    for item in text:
        if item not in lst:
            result.append(item)

    return result

df['Resume']=df['Resume'].apply(remove_freq_words)

"""**lemmatization**"""

from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize,pos_tag

def lemmatization(text):

    result=[]
    wordnet = WordNetLemmatizer()
    for token,tag in pos_tag(text):
        pos=tag[0].lower()

        if pos not in ['a', 'r', 'n', 'v']:
            pos='n'

        result.append(wordnet.lemmatize(token,pos))

    return result

df ['Resume']=df['Resume'].apply(lemmatization)
df.head()

"""removal of special characters"""

import re

cols_to_clean = ['Category', 'Resume']

for col in cols_to_clean:
    df[col] = df[col].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s_]+', '', str(x)))

for col in df.columns:
    df[col] = df[col].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s_]+', '', str(x)))

df.to_csv('cleaned_dataset.csv', index=False)

df.head()

skills_list = ['python', 'java', 'sql', 'machine learning', 'data analysis','data science','testing']

def check_skills(text, skills):
    for skill in skills:
        if skill.lower() in text.lower():
            return True
    return False

df['skills_present'] = df['Resume'].apply(lambda x: check_skills(x, skills_list))

skills_df = df[df['skills_present']]

skills_df.to_csv('skills_dataset.csv', index=False)

df

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer().fit(df.Resume)

print(len(vectorizer.vocabulary_))

new = df.Resume[5]
print(new)

vec5 = vectorizer.transform([new])
print(vec5)
print(vec5.shape)

news_vec = vectorizer.transform(df.Resume)

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(news_vec)

tfidf5 = tfidf_transformer.transform(vec5)
print(tfidf5)

news_tfidf = tfidf_transformer.transform(news_vec)
print(news_tfidf.shape)

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split


# Split the dataset into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(
    df['Resume'], df['Category'], test_size=0.2, random_state=42)

# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)

from sklearn.metrics import precision_score, recall_score, f1_score
clf = MultinomialNB()
clf.fit(train_counts, train_labels)

# Evaluate the performance of the classifier on the testing data
test_counts = vectorizer.transform(test_data)
accuracy = clf.score(test_counts, test_labels)


print('Accuracy:', accuracy)

# Classify a new resume based on its skill set
new_resume = 'skill program language python panda numpy scipy scikitlearn matplotlib sql java javascriptjquery machine learning regression svm nave bayes knn random forest decision tree boost technique cluster analysis word embedding sentiment analysis natural language processing dimensionality reduction topic modelling lda nmf pca neural net database visualization mysql sqlserver cassandra hbase elasticsearch d3js dcjs plotly kibana matplotlib ggplot tableau others regular expression html cs angular 6 logstash kafka python flask git docker computer vision open cv and understanding of deep learningeducation detail data science assurance associate data science assurance associate ernst young llp skill detail javascript exprience 24 month jquery exprience 24 month python exprience 24 monthscompany detail company ernst young llp description fraud investigation and dispute service assurance technology assist review tar technology assist review assist in accelerate the review process and run analytics and generate report core member of a team help in develop automate review platform tool from scratch for assist e discovery domain this tool implement predictive coding and topic modelling by automate review result in reduced labor cost and time spend during the lawyer review understand the end to end flow of the solution do research and development for classification model predictive analysis and mining of the information present in text data work on analyze the output and precision monitoring for the entire tool tar assist in predictive coding topic modelling from the evidence by follow ey standard develop the classifier model in order to identify red flag and fraudrelated issue tool technology python scikitlearn tfidf word2vec doc2vec cosine similarity nave bayes lda nmf for topic modelling vader and text blob for sentiment analysis matplot lib tableau dashboard for report multiple data science and analytic project usa client text analytics motor vehicle customer review data receive customer feedback survey data for past one year perform sentiment positive negative neutral and time series analysis on customer comment across all 4 category create heat map of term by survey category base on frequency of word extract positive and negative word across all the survey category and plot word cloud create customize tableau dashboard for effective reporting and visualization chatbot develop a user friendly chatbot for one of our product which handle simple question about hour of operation reservation option and so on this chat bot serf entire product relate question give overview of tool via qa platform and also give recommendation response so that user question to build chain of relevant answer this too have intelligence to build the pipeline of question a per user requirement and ask the relevant recommended question tool technology python natural language processing nltk spacy topic modelling sentiment analysis word embedding scikitlearn javascriptjquery sqlserver information governance organization to make informed decision about all of the information they store the integrated information governance portfolio synthesize intelligence across unstructured data source and facilitate action to ensure organization be best position to counter information risk scan data from multiple source of format and parse different file format extract meta data information push result for index elastic search and create customize interactive dashboard use kibana preform rot analysis on the data which give information of data which help identify content that be either redundant outdated or trivial preform fulltext search analysis on elastic search with predefined method which can tag a pii personally identifiable information social security number address name etc which frequently target during cyberattacks tool technology python flask elastic search kibana fraud analytic platform fraud analytics and investigative platform to review all red flag case x80 fap be a fraud analytics and investigative platform with inbuilt case manager and suite of analytics for various erp system it can be use by client to interrogate their accounting system for identify the anomaly which can be indicator of fraud by run advanced analytics tool technology html javascript sqlserver jquery cs bootstrap nodejs d3js dcjs	True'
'data science	education detail may 2013 to may 2017 be uitrgpv data scientist data scientist matelabs skill detail python exprience le than 1 year month statsmodels exprience 12 month aws exprience le than 1 year month machine learning exprience le than 1 year month sklearn exprience le than 1 year month scipy exprience le than 1 year month keras exprience le than 1 year monthscompany detail company matelabs description ml platform for business professional dummy and enthusiast 60a koramangala 5th block achievementstasks behind sukh sagar  '
new_counts = vectorizer.transform([new_resume])
predicted_category = clf.predict(new_counts)[0]
print('Predicted category:', predicted_category)

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Preprocess the data (handle missing values, encode categorical variables, scale numerical features, etc.)

# Split the dataset into features (X) and labels (y)
X = df.drop('Category', axis=1)
y = df['Category']

# Split the data into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(
    df['Resume'], df['Category'], test_size=0.2, random_state=42)

# Create DMatrix
dtrain = xgb.DMatrix(train_data, label=train_labels)
dtest = xgb.DMatrix(test_data, label=test_labels)

# Set XGBoost parameters
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss'
}

# Train the model
num_rounds = 100
xgb_model = xgb.train(params, dtrain, num_rounds)

# Make predictions
y_pred = xgb_model.predict(dtest)
y_pred_binary = [round(value) for value in y_pred]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_binary)
print("Accuracy:", accuracy)

"""Random forest

"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score



# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)
test_counts = vectorizer.transform(test_data)

# Define the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
gb_precision = precision_score(test_labels, predicted_labels,average='weighted')
gb_recall = recall_score(test_labels, predicted_labels,average='weighted')

f1_gb=f1_score(test_labels, predicted_labels,average='weighted')
print('Random Forest classifier:')
print('Precision:', gb_precision)
print('Recall:', gb_recall)
print('F1 Score:', f1_gb)

print('Accuracy:', accuracy)

# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_counts = vectorizer.transform([new_resume])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

"""pos tagging

"""

import pandas as pd
import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score



# Split the dataset into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(
    df['Resume'], df['Category'], test_size=0.2, random_state=42)

# Define a function to extract the nouns and adjectives from a text
def extract_features(text):
    tokens = word_tokenize(text)
    tagged_tokens = pos_tag(tokens)
    return [token for token, pos in tagged_tokens if pos.startswith('N') or pos.startswith('J')]

# Extract the features from the training and testing data
train_features = [extract_features(text) for text in train_data]
test_features = [extract_features(text) for text in test_data]

# Convert the features into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform([' '.join(features) for features in train_features])
test_counts = vectorizer.transform([' '.join(features) for features in test_features])

# Define the Naive Bayes model
model = MultinomialNB()

# Train the Naive Bayes model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
print('Accuracy:', accuracy)

# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_features = extract_features(new_resume)
new_counts = vectorizer.transform([' '.join(new_features)])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

print(train_data)

print(test_data)

print(train_labels)

print(test_labels)

"""**ADABOOST CLASSIFIER**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)
test_counts = vectorizer.transform(test_data)

# Define the Random Forest model
model = AdaBoostClassifier(n_estimators=100, random_state=42)


# Train the Random Forest model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
precision = precision_score(test_labels, predicted_labels, average='weighted')
recall = recall_score(test_labels, predicted_labels, average='weighted')
f1 = f1_score(test_labels, predicted_labels, average='weighted')

print('AdaBoost classifier:')
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
print('Accuracy:', accuracy)

# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_counts = vectorizer.transform([new_resume])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

"""**KNN CLASSIFIER**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)
test_counts = vectorizer.transform(test_data)

# Define the Random Forest model
model = KNeighborsClassifier(n_neighbors=5)


# Train the Random Forest model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
precision = precision_score(test_labels, predicted_labels, average='weighted')
recall = recall_score(test_labels, predicted_labels, average='weighted')
f1 = f1_score(test_labels, predicted_labels, average='weighted')

print('KNN classifier:')
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
print('Accuracy:', accuracy)


# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_counts = vectorizer.transform([new_resume])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

"""**SVM CLASSIFIER**"""

from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)
test_counts = vectorizer.transform(test_data)

# Define the Random Forest model
model = SVC(kernel='linear', decision_function_shape='ovr')


# Train the Random Forest model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
precision = precision_score(test_labels, predicted_labels, average='weighted')
recall = recall_score(test_labels, predicted_labels, average='weighted')
f1 = f1_score(test_labels, predicted_labels, average='weighted')

print('SVM classifier:')
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
print('Accuracy:', accuracy)



# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_counts = vectorizer.transform([new_resume])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

"""**DECISION TREE CLASSIFIER**"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score



# Split the dataset into features (X) and labels (y)
X = df['Resume']
y = df['Category']

# Create a TF-IDF vectorizer to convert text into numerical features
vectorizer = CountVectorizer()
X_transformed = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

# Create a Decision Tree classifier
dt = DecisionTreeClassifier()

# Train the classifier
dt.fit(X_train, y_train)

# Make predictions
y_pred = dt.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict skills and category for a specific candidate
candidate_resume = ['Java, SQL, Agile, Scrum']
candidate_resume_transformed = vectorizer.transform(candidate_resume)
predicted_category = dt.predict(candidate_resume_transformed)[0]

print("Predicted Category:", predicted_category)

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import MaxAbsScaler


# Convert the text data into a matrix of token counts
vectorizer = CountVectorizer()
train_counts = vectorizer.fit_transform(train_data)
test_counts = vectorizer.transform(test_data)

# Scale the data using MaxAbsScaler
scaler = MaxAbsScaler()
train_counts_scaled = scaler.fit_transform(train_counts)
test_counts_scaled = scaler.transform(test_counts)

# Define the Random Forest model
model = LogisticRegression(max_iter=1000)

# Train the Random Forest model on the training data
model.fit(train_counts, train_labels)

# Evaluate the performance of the model on the testing data
predicted_labels = model.predict(test_counts)
accuracy = accuracy_score(test_labels, predicted_labels)
precision = precision_score(test_labels, predicted_labels, average='weighted')
recall = recall_score(test_labels, predicted_labels, average='weighted')
f1 = f1_score(test_labels, predicted_labels, average='weighted')


print('Logistic Regression classifier:')
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
print('Accuracy:', accuracy)




# Classify a new resume based on its skill set
new_resume = 'Java, Python, SQL, Agile, Scrum'
new_counts = vectorizer.transform([new_resume])
predicted_category = model.predict(new_counts)[0]
print('Predicted category:', predicted_category)

"""**XG BOSST CLASSIFIER**"""

